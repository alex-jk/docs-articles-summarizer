{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, MarianMTModel, MarianTokenizer\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from langdetect import detect\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "/usr/local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def load_model():\n",
    "    model_directory = \"t5-base\"  # Using T5 for multilingual support\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_directory)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_directory)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_models():\n",
    "    # Load translation models\n",
    "    translation_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "    translation_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "    return translation_model, translation_tokenizer\n",
    "\n",
    "translation_model, translation_tokenizer = load_translation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, src_lang):\n",
    "    # Translate text to English\n",
    "    src_lang = src_lang.lower()\n",
    "    if src_lang == \"zh-cn\":\n",
    "        src_lang = \"zh\"\n",
    "    translation_input = translation_tokenizer.prepare_seq2seq_batch([text], src_lang=src_lang, tgt_lang=\"en\", return_tensors=\"pt\")\n",
    "    translated_ids = translation_model.generate(**translation_input)\n",
    "    translated_text = translation_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and extra whitespace\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, prompts):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    combined_text = f\"summarize: {cleaned_text}\"\n",
    "    if prompts:\n",
    "        combined_text += \" \" + \" \".join(prompts)\n",
    "    \n",
    "    tokenized_text = tokenizer.encode(combined_text, return_tensors=\"pt\", max_length=512, truncation=True, padding=True)\n",
    "    \n",
    "    summary_ids = model.generate(tokenized_text, max_length=150, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pdf(file):\n",
    "    pdf_document = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    return file.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file, lang):\n",
    "    image = Image.open(file)\n",
    "    image_np = np.array(image)  # Convert PIL Image to numpy array\n",
    "    \n",
    "    # Language groups\n",
    "    latin_languages = ['en', 'fr', 'de', 'es', 'it', 'pt']\n",
    "    cyrillic_languages = ['ru', 'rs_cyrillic', 'be', 'bg', 'uk', 'mn', 'en']\n",
    "    ja_ko_zh_languages = ['ja', 'ko', 'zh-cn', 'zh-tw', 'en']\n",
    "    \n",
    "    if lang in ['ja', 'ko', 'zh-cn', 'zh-tw']:\n",
    "        reader = easyocr.Reader(ja_ko_zh_languages)\n",
    "    elif lang in cyrillic_languages:\n",
    "        reader = easyocr.Reader(cyrillic_languages)\n",
    "    else:\n",
    "        reader = easyocr.Reader(latin_languages)\n",
    "    \n",
    "    result = reader.readtext(image_np, detail=0)\n",
    "    \n",
    "    text = ' '.join(result)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    lang = detect(text)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gabriel2006.pdf', 'NPR2-42-120.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the folder containing the PDFs\n",
    "pdf_folder = 'pdf_files'\n",
    "\n",
    "# List all files in the pdf_folder and filter to include only PDFs\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
