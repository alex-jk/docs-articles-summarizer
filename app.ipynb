{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, MarianMTModel, MarianTokenizer\n",
    "import fitz  # PyMuPDF\n",
    "import os\n",
    "import re\n",
    "from langdetect import detect\n",
    "import easyocr\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    model_directory = \"t5-base\"  # Using T5 for multilingual support\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_directory)\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_directory)\n",
    "    return model, tokenizer\n",
    "\n",
    "model, tokenizer = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_translation_models():\n",
    "    # Load translation models\n",
    "    translation_model = MarianMTModel.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "    translation_tokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-mul-en\")\n",
    "    return translation_model, translation_tokenizer\n",
    "\n",
    "translation_model, translation_tokenizer = load_translation_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text, src_lang):\n",
    "    # Translate text to English\n",
    "    src_lang = src_lang.lower()\n",
    "    if src_lang == \"zh-cn\":\n",
    "        src_lang = \"zh\"\n",
    "    translation_input = translation_tokenizer.prepare_seq2seq_batch([text], src_lang=src_lang, tgt_lang=\"en\", return_tensors=\"pt\")\n",
    "    translated_ids = translation_model.generate(**translation_input)\n",
    "    translated_text = translation_tokenizer.decode(translated_ids[0], skip_special_tokens=True)\n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Remove special characters and extra whitespace\n",
    "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text)\n",
    "    cleaned_text = cleaned_text.strip()\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text, prompts=None):\n",
    "    cleaned_text = preprocess_text(text)\n",
    "    \n",
    "    # Create a structured input text with a separator\n",
    "    combined_text = f\"summarize: {cleaned_text}\"\n",
    "    if prompts:\n",
    "        prompt_text = \" ### \".join(prompts)  # Separate each prompt with ###\n",
    "        combined_text = f\"{prompt_text} ### {cleaned_text}\"\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    tokenized_text = tokenizer.encode(\n",
    "        combined_text, \n",
    "        return_tensors=\"pt\", \n",
    "        max_length=1024,  # Increase max_length for larger input context\n",
    "        truncation=True, \n",
    "        padding=True\n",
    "    )\n",
    "    \n",
    "    # Generate the summary with adjusted parameters\n",
    "    summary_ids = model.generate(\n",
    "        tokenized_text,\n",
    "        max_length=300,\n",
    "        num_beams=6,\n",
    "        repetition_penalty=2.0,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    # Decode the generated tokens into the final summary text\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pdfplumber'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m         text \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mget_text()\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m text\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdfplumber\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_pdf_with_pdfplumber\u001b[39m(file_path):\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Extracts text from a PDF using pdfplumber, maintaining the reading order and layout.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pdfplumber'"
     ]
    }
   ],
   "source": [
    "def read_pdf(file):\n",
    "    pdf_document = fitz.open(stream=file.read(), filetype=\"pdf\")\n",
    "    text = \"\"\n",
    "    for page_num in range(len(pdf_document)):\n",
    "        page = pdf_document.load_page(page_num)\n",
    "        text += page.get_text()\n",
    "    return text\n",
    "\n",
    "import pdfplumber\n",
    "\n",
    "def read_pdf_with_pdfplumber(file_path):\n",
    "    \"\"\"Extracts text from a PDF using pdfplumber, maintaining the reading order and layout.\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            # Extract text while preserving the layout of the PDF\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:  # Only add if text is extracted correctly\n",
    "                text += page_text + \"\\n\"\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_txt(file):\n",
    "    return file.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image(file, lang):\n",
    "    image = Image.open(file)\n",
    "    image_np = np.array(image)  # Convert PIL Image to numpy array\n",
    "    \n",
    "    # Language groups\n",
    "    latin_languages = ['en', 'fr', 'de', 'es', 'it', 'pt']\n",
    "    cyrillic_languages = ['ru', 'rs_cyrillic', 'be', 'bg', 'uk', 'mn', 'en']\n",
    "    ja_ko_zh_languages = ['ja', 'ko', 'zh-cn', 'zh-tw', 'en']\n",
    "    \n",
    "    if lang in ['ja', 'ko', 'zh-cn', 'zh-tw']:\n",
    "        reader = easyocr.Reader(ja_ko_zh_languages)\n",
    "    elif lang in cyrillic_languages:\n",
    "        reader = easyocr.Reader(cyrillic_languages)\n",
    "    else:\n",
    "        reader = easyocr.Reader(latin_languages)\n",
    "    \n",
    "    result = reader.readtext(image_np, detail=0)\n",
    "    \n",
    "    text = ' '.join(result)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    lang = detect(text)\n",
    "    return lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gabriel2006.pdf', 'NPR2-42-120.pdf']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Define the path to the folder containing the PDFs\n",
    "pdf_folder = 'pdf_files'\n",
    "\n",
    "# List all files in the pdf_folder and filter to include only PDFs\n",
    "pdf_files = [f for f in os.listdir(pdf_folder) if f.endswith('.pdf')]\n",
    "print(pdf_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Effects of lamotrigine on unipolar depression.\",\n",
    "    \"Impact of lamotrigine on unipolar depression.\",\n",
    "    \"Key findings related to lamotrigine in treating unipolar depression.\",\n",
    "    \"Outcomes and statistics related to lamotrigine and unipolar depression.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ### Key findings and statistics related to lamotrigine in treating unipolar depression. ### DEPRESSION AND ANXIETY 23485488 2006 Brief Report LAMOTRIGINE ADJUNCTIVE TREATMENT IN RESISTANT UNIPOLAR DEPRESSION A small number of reports suggest some efficacy of lamotrigine in treating unipolar depression\n",
      "\n",
      " ### Key findings related to lamotrigine in treating unipolar depression. ### Outcomes and statistics related to lamotrigine and unipolar depression. ### 120 Neuropsychopharmacology Reports 202242120123 wileyonlinelibrarycomjournalnppr 1 INTRODUCTION Persistent depressive disorder PDD was first introduced in the Diagnostic and Statistical Manual of Mental Disorders 5th edition DSM5 which encompasses numerous different conditions in cluding dysthy\n"
     ]
    }
   ],
   "source": [
    "# Initialize a list to store the summaries\n",
    "summaries = []\n",
    "\n",
    "# Iterate through each PDF file and generate a summary\n",
    "for pdf_file in pdf_files:\n",
    "    file_path = os.path.join(pdf_folder, pdf_file)\n",
    "    \n",
    "    # Open and read the PDF file\n",
    "    with open(file_path, 'rb') as file:\n",
    "        file_text = read_pdf(file)\n",
    "    \n",
    "    # Generate a summary for the current PDF file using optional prompts\n",
    "    summary = summarize_text(file_text, prompts)\n",
    "    print(\"\\n\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_extracted_text(text):\n",
    "    \"\"\"\n",
    "    Cleans the extracted text by removing unwanted line breaks, fixing broken sentences,\n",
    "    and handling common text extraction issues.\n",
    "    \"\"\"\n",
    "    # Remove unwanted newlines that don't indicate paragraph breaks\n",
    "    text = re.sub(r\"(?<!\\n)\\n(?!\\n)\", \" \", text)  # Replace single newlines with a space\n",
    "\n",
    "    # Remove multiple newlines and excessive spaces\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "\n",
    "    # Fix hyphenated line breaks (e.g., \"depres-\\nsion\" becomes \"depression\")\n",
    "    text = re.sub(r\"(\\w+)-\\s*\\n\\s*(\\w+)\", r\"\\1\\2\", text)\n",
    "\n",
    "    # Optionally, fix other common formatting issues if needed\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_relevant_sections(text, keyword=\"lamotrigine\"):\n",
    "    \"\"\"Extract paragraphs or sentences containing the keyword from the text.\"\"\"\n",
    "    relevant_sections = []\n",
    "    for paragraph in text.split('\\n'):\n",
    "        if keyword.lower() in paragraph.lower():\n",
    "            relevant_sections.append(paragraph)\n",
    "    return \" \".join(relevant_sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_pdf_with_pdfplumber' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m curr_file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pdf_folder, pdf_files[ind])  \u001b[38;5;66;03m# First file\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(curr_file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m----> 7\u001b[0m     curr_file_text \u001b[38;5;241m=\u001b[39m \u001b[43mread_pdf_with_pdfplumber\u001b[49m(file)\n\u001b[1;32m      8\u001b[0m     extracted_text \u001b[38;5;241m=\u001b[39m extract_relevant_sections(curr_file_text, keyword\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlamotrigine\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m     cleaned_text \u001b[38;5;241m=\u001b[39m clean_extracted_text(extracted_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_pdf_with_pdfplumber' is not defined"
     ]
    }
   ],
   "source": [
    "pdf_folder = \"pdf_files\"  # Replace with your actual folder name\n",
    "\n",
    "# Select the first PDF file and process it\n",
    "ind = 0\n",
    "curr_file_path = os.path.join(pdf_folder, pdf_files[ind])  # First file\n",
    "with open(curr_file_path, 'rb') as file:\n",
    "    curr_file_text = read_pdf_with_pdfplumber(file)\n",
    "    extracted_text = extract_relevant_sections(curr_file_text, keyword=\"lamotrigine\")\n",
    "    cleaned_text = clean_extracted_text(extracted_text)\n",
    "\n",
    "# print('\\n', curr_file_text)\n",
    "print('\\n', cleaned_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
